<!DOCTYPE html>
<html lang="en-GB">
<head>
  <meta charset="utf-8" />
  <title>LexiMind Assistant – Technical Overview</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <!-- Global docs stylesheet -->
  <link rel="stylesheet" href="../styles.css">

  
</head>

<body>
  <header>
    <span class="tag">Conversational AI</span>
    <span class="tag">NLP</span>
    <span class="tag">Internal Tooling</span>

    <h1>LexiMind Assistant Technical Overview</h1>
    <nav class="docs-toc docs-toc-dropdown">
      <details open>
        <summary>On this page</summary>
        <ol>
          <li><a href="#1-system-architecture">1. System Architecture</a></li>
          <li><a href="#2-nlp-pipeline">2. NLP Pipeline</a></li>
          <li><a href="#3-api-design-usage">3. API Design &amp; Usage</a></li>
          <li><a href="#4-core-workflows">4. Core Workflows</a></li>
          <li><a href="#5-performance-evaluation">5. Performance &amp; Evaluation</a></li>
          <li><a href="#6-error-handling-observability">6. Error Handling &amp; Observability</a></li>
          <li><a href="#7-extensibility-roadmap">7. Extensibility &amp; Roadmap</a></li>
          <li><a href="#8-security-compliance">8. Security &amp; Compliance</a></li>
          <li><a href="#9-appendix">9. Appendix</a></li>
        </ol>
      </details>
    </nav>
    <section class="doc-meta-grid">
      <h2 class="doc-meta-title">Role &amp; Outcomes</h2>
      <div class="doc-meta-row">
        <div class="doc-meta-item">
          <h3>Overview</h3>
          <p>This document describes <strong>LexiMind Assistant Technical Overview</strong> and is written from the perspective of a technical writer supporting engineers, product, and operations.</p>
        </div>
        <div class="doc-meta-item">
          <h3>My Role</h3>
          <p>Planned the structure, authored the documentation, and aligned the content with stakeholders so it can be used as a repeatable reference and onboarding asset.</p>
        </div>
      </div>
      <div class="doc-meta-row">
        <div class="doc-meta-item">
          <h3>Audience</h3>
          <p>Primarily software engineers, data/ML practitioners, and product or operations teams who need a clear view of behaviour, integration steps, and edge cases.</p>
        </div>
        <div class="doc-meta-item">
          <h3>Scope</h3>
          <p>In scope: product behaviour, configuration, integration flows, and operational considerations. Out of scope: commercial terms, team processes, and non-technical marketing copy.</p>
        </div>
      </div>
      <div class="doc-meta-row">
        <div class="doc-meta-item">
          <h3>Prerequisites</h3>
          <p>Comfortable reading technical documentation and basic familiarity with the surrounding stack (e.g. APIs, data pipelines, dashboards, or game engines, depending on context).</p>
        </div>
        <div class="doc-meta-item">
          <h3>Impact</h3>
          <p>Intended to reduce back-and-forth clarification, speed up onboarding, and make future changes safer by giving teams a single, well-structured source of truth.</p>
        </div>
      </div>
    </section>


    <p>
      LexiMind Assistant is an internal-facing, AI-powered writing and support assistant designed to help teams draft documentation,
      summarise customer tickets, and generate knowledge base content with consistent tone and structure.
      This document provides a senior-level technical overview of the system architecture, NLP pipeline, APIs, workflows,
      performance characteristics, and future roadmap.
    </p>

    <div class="meta-grid">
      <div>
        <span class="meta-label">Owner:</span> ML Platform / Documentation Tools
      </div>
      <div>
        <span class="meta-label">Primary Users:</span> Technical writers, Support agents, Product teams
      </div>
      <div>
        <span class="meta-label">Status:</span> Production, v1.3
      </div>
      <div>
        <span class="meta-label">Last Updated:</span> 16 Nov 2025
      </div>
    </div>

    <div class="callout">
      <strong>Scope.</strong> This page is written for engineering, ML, and platform stakeholders who need to understand how
      LexiMind Assistant works end-to-end: from input handling and language modelling, through evaluation and observability, to
      security and extensibility.
    </div>
  </header>

  <main>
    <!-- 1. High-Level Architecture -->
    <section id="architecture">
      <h2 id="1-system-architecture">1. System Architecture</h2>

      <p>
        LexiMind Assistant follows a modular, service-oriented architecture. The assistant is exposed through a stateless HTTP API,
        backed by a dedicated inference layer, prompt and metadata services, and shared observability infrastructure.
      </p>

      <h3>1.1 Component Overview</h3>

      <ul>
        <li><strong>API Gateway</strong> – entrypoint for all client traffic (REST + future gRPC), responsible for authentication, rate limiting, and request validation.</li>
        <li><strong>Orchestration Service</strong> – coordinates the full request lifecycle: routing, prompt construction, tool selection, and post-processing.</li>
        <li><strong>NLP Inference Layer</strong> – hosts one or more large language models (LLMs) optimised for writing, summarisation, and Q&amp;A.</li>
        <li><strong>Context &amp; Retrieval Service</strong> – provides document search over internal knowledge (Confluence, Jira, previous tickets) using embeddings + vector search.</li>
        <li><strong>Content Policy &amp; Safety Guard</strong> – performs input and output filtering for PII, toxicity, and policy violations.</li>
        <li><strong>Telemetry &amp; Feedback Store</strong> – logs events, traces, and explicit user feedback for evaluation and continuous improvement.</li>
      </ul>

      <div class="diagram">
        <div class="diagram-title">Figure-1 High-Level Architecture</div>
        <pre><code>User
  |
  v
[ Client UI ]  (Docs editor, support console)
  |
  v
[ API Gateway ]
  | auth / rate limit / validation
  v
[ Orchestration Service ]
  |-- calls Context Service (RAG)
  |-- assembles prompts
  |-- selects model & tools
  v
[ NLP Inference Layer ]
  |
  v
[ Safety Guard ] ---> [ Telemetry & Feedback ]
  |
  v
Response to client</code></pre>
        <p>
          In production, the API Gateway, Orchestration Service, and Context Service are deployed as independently scalable
          Kubernetes workloads. The inference layer is hosted on a separate GPU pool with autoscaling based on queue depth and
          token throughput.
        </p>
      </div>
    </section>

    <!-- 2. NLP Pipeline -->
    <section id="nlp-pipeline">
      <h2 id="2-nlp-pipeline">2. NLP Pipeline</h2>

      <p>
        The NLP pipeline transforms raw user input into a safe, context-aware response. Each stage is observable and can be
        individually tuned or swapped out.
      </p>

      <h3>2.1 Pipeline Stages</h3>

      <ol>
        <li>
          <strong>Input Normalisation</strong><br />
          Trims whitespace, enforces UTF-8, normalises line endings, and detects language.
        </li>
        <li>
          <strong>Intent &amp; Task Classification</strong><br />
          Classifies the request into high-level tasks (e.g. <em>summarise</em>, <em>rewrite</em>, <em>generate documentation</em>, <em>answer question</em>).
        </li>
        <li>
          <strong>Context Retrieval (RAG)</strong><br />
          If the task requires organisation-specific knowledge, the system queries the vector store using an embedding model.
        </li>
        <li>
          <strong>Prompt Assembly</strong><br />
          Combines system instructions, user input, retrieved context, and guardrails into a single prompt or structured messages.
        </li>
        <li>
          <strong>LLM Inference</strong><br />
          Runs on the selected model (e.g. <code>leximind-gen-1</code> for drafting, <code>leximind-summariser</code> for compression).
        </li>
        <li>
          <strong>Post-Processing</strong><br />
          Enforces formatting (e.g. markdown/HTML), checks safety constraints, and ensures answer completeness.
        </li>
      </ol>

      <div class="diagram">
        <div class="diagram-title">Figure 2 – NLP Pipeline</div>
        <pre><code>Input
  -> Normalise &amp; detect language
  -> Classify task (summarise / draft / explain)
  -> Fetch contextual docs (RAG)
  -> Assemble system + user + docs into prompt
  -> Run LLM inference
  -> Safety &amp; formatting checks
  -> Return response</code></pre>
      </div>

      <h3>2.2 Models Used</h3>

      <ul class="pill-list">
        <li>Generation: leximind-gen-1 (48B params)</li>
        <li>Summarisation: leximind-sum-1</li>
        <li>Embeddings: leximind-embed-qa</li>
        <li>Toxicity / PII classifiers: internal BERT-based models</li>
      </ul>
    </section>

    <!-- 3. API Design -->
    <section id="api">
      <h2 id="3-api-design-usage">3. API Design &amp; Usage</h2>

      <p>
        LexiMind Assistant is exposed as a JSON-over-HTTP API. Clients should treat it as a stateless service:
        all context required for a call must be provided per request.
      </p>

      <h3>3.1 Generate Response Endpoint</h3>

      <div class="section-label">Endpoint</div>
      <p><code>POST /v1/assistant/generate</code></p>

      <div class="section-label">Request Body</div>
      <pre><code class="language-json">{
  "trace_id": "9f4a0ae0-22b5-11ef-9f1d-0242ac120003",
  "task": "draft_documentation",
  "input": {
    "prompt": "Write a model card for our LexiWrite Gen-1 model.",
    "tone": "formal",
    "format": "markdown"
  },
  "context": {
    "project_id": "lexiwrite-gen1",
    "kb_refs": ["confluence:LEXI-23", "jira:LEXI-1021"]
  },
  "constraints": {
    "max_tokens": 800,
    "temperature": 0.3
  },
  "metadata": {
    "caller": "docs-portal",
    "user_role": "technical_writer"
  }
}</code></pre>

      <div class="section-label">Response</div>
      <pre><code class="language-json">{
  "trace_id": "9f4a0ae0-22b5-11ef-9f1d-0242ac120003",
  "model": "leximind-gen-1",
  "usage": {
    "prompt_tokens": 524,
    "completion_tokens": 612,
    "total_tokens": 1136,
    "latency_ms": 892
  },
  "output": {
    "format": "markdown",
    "content": "## Model Overview\\nLexiWrite Gen-1 is a large language model..."
  },
  "safety": {
    "blocked": false,
    "flags": []
  }
}</code></pre>

      <h3>3.2 Error Responses</h3>

      <p>All errors follow a consistent schema:</p>

      <pre><code class="language-json">{
  "error": {
    "code": "RATE_LIMIT_EXCEEDED",
    "message": "Too many requests from this API key.",
    "retry_after_ms": 15000,
    "trace_id": "..."
  }
}</code></pre>

      <p>See <a href="#error-handling">Error Handling &amp; Observability</a> for full list of error codes.</p>
    </section>

    <!-- 4. Workflows -->
    <section id="workflows">
      <h2 id="4-core-workflows">4. Core Workflows</h2>

      <h3>4.1 Documentation Drafting Workflow</h3>

      <ol>
        <li>Technical writer selects a template in the docs portal (e.g. “Model Card”, “API Guide”).</li>
        <li>Portal sends a <code>draft_documentation</code> request to LexiMind Assistant with project metadata.</li>
        <li>Assistant pulls relevant design docs and tickets via the Context Service.</li>
        <li>LLM generates a first draft conforming to the selected template.</li>
        <li>Writer reviews, edits, and can send a <code>rewrite</code> or <code>expand_section</code> request for specific sections.</li>
        <li>Final approved draft is exported back to Markdown and pushed to the docs repository.</li>
      </ol>

      <h3>4.2 Ticket Summarisation Workflow</h3>

      <ol>
        <li>Support agent opens a long multi-comment ticket in the helpdesk console.</li>
        <li>Client sends <code>summarise_ticket</code> request including full ticket conversation.</li>
        <li>Assistant generates: (a) short summary, (b) root cause guess, (c) next recommended action.</li>
        <li>Agent can accept, edit, or regenerate the summary. Feedback is logged per trace.</li>
      </ol>
    </section>

    <!-- 5. Performance -->
    <section id="performance">
      <h2 id="5-performance-evaluation">5. Performance &amp; Evaluation</h2>

      <p>
        Performance is tracked across three dimensions: latency, reliability, and response quality.
        Metrics are ingested into Prometheus and visualised in Grafana dashboards.
      </p>

      <h3>5.1 Latency &amp; Throughput</h3>

      <table>
        <thead>
          <tr>
            <th>Metric</th>
            <th>Target (P95)</th>
            <th>Current (Rolling 7 days)</th>
            <th>Notes</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>End-to-end latency (docs drafting)</td>
            <td class="metric-good">&lt; 1,500 ms</td>
            <td>1,320 ms</td>
            <td>Includes retrieval + inference on GPU.</td>
          </tr>
          <tr>
            <td>End-to-end latency (summaries)</td>
            <td class="metric-good">&lt; 900 ms</td>
            <td>780 ms</td>
            <td>Shorter prompts, summarisation model.</td>
          </tr>
          <tr>
            <td>Error rate (5xx)</td>
            <td class="metric-good">&lt; 0.5%</td>
            <td>0.23%</td>
            <td>Mostly transient timeouts from upstream retrieval.</td>
          </tr>
          <tr>
            <td>Token throughput</td>
            <td class="metric-watch">&gt; 50k tokens/s</td>
            <td>43k tokens/s</td>
            <td>Scaling improvements planned in v1.4.</td>
          </tr>
        </tbody>
      </table>

      <h3>5.2 Quality Evaluation</h3>

      <ul>
        <li><strong>Human review panels.</strong> Monthly review of sampled outputs by tech writers and support leads.</li>
        <li><strong>Task-specific scores.</strong> Accuracy, completeness, formatting correctness per template.</li>
        <li><strong>User feedback loop.</strong> Thumb up/down and free-text comments wired into the feedback store.</li>
      </ul>
    </section>

    <!-- 6. Error Handling -->
    <section id="error-handling">
      <h2 id="6-error-handling-observability">6. Error Handling &amp; Observability</h2>

      <p>
        The system is designed to fail loudly and traceably rather than silently degrade. All requests are tagged with
        a <code>trace_id</code> propagated across services.
      </p>

      <h3>6.1 Error Categories</h3>

      <table>
        <thead>
          <tr>
            <th>Code</th>
            <th>Category</th>
            <th>Typical Cause</th>
            <th>Client Action</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td><code>INVALID_REQUEST</code></td>
            <td>4xx</td>
            <td>Missing fields, unsupported task, invalid JSON.</td>
            <td>Fix request payload and retry.</td>
          </tr>
          <tr>
            <td><code>UNAUTHENTICATED</code></td>
            <td>4xx</td>
            <td>Missing or invalid API key / token.</td>
            <td>Refresh credentials; check IAM configuration.</td>
          </tr>
          <tr>
            <td><code>RATE_LIMIT_EXCEEDED</code></td>
            <td>4xx</td>
            <td>Caller exceeding configured QPS or token limit.</td>
            <td>Back off and retry after <code>retry_after_ms</code>.</td>
          </tr>
          <tr>
            <td><code>INFERENCE_BACKEND_UNAVAILABLE</code></td>
            <td>5xx</td>
            <td>GPU pool unavailable, deployment roll-out, or network issues.</td>
            <td>Retry with exponential backoff; contact platform team if persistent.</td>
          </tr>
          <tr>
            <td><code>SAFETY_VIOLATION</code></td>
            <td>4xx</td>
            <td>Input or generated content flagged by content safety policies.</td>
            <td>Adjust prompt content; some topics cannot be served.</td>
          </tr>
        </tbody>
      </table>

      <h3>6.2 Observability</h3>

      <ul>
        <li><strong>Logging.</strong> Structured logs in JSON, redacting PII tokens where possible.</li>
        <li><strong>Tracing.</strong> OpenTelemetry spans for gateway, orchestration, retrieval, and inference.</li>
        <li><strong>Metrics.</strong> Per-model latency, token usage, rate limit breaches, safety block rates.</li>
      </ul>
    </section>

    <!-- 7. Extensibility & Roadmap -->
    <section id="extensibility">
      <h2 id="7-extensibility-roadmap">7. Extensibility &amp; Roadmap</h2>

      <h3>7.1 Extension Points</h3>

      <ul>
        <li>
          <strong>Tooling / Functions API.</strong> The orchestration layer supports plugging in tools such as
          glossary lookups, style guides, or internal APIs (e.g. “fetch Jira ticket summary”).
        </li>
        <li>
          <strong>Additional models.</strong> New models can be registered behind the same API contract with
          routing rules based on task, latency budget, or cost.
        </li>
        <li>
          <strong>Templates.</strong> Documentation and email templates are defined declaratively, allowing
          non-engineers to add new content types.
        </li>
      </ul>

      <h3>7.2 Roadmap (Next 2–3 Releases)</h3>

      <ul>
        <li>Multi-language support (DE/ES) for summarisation and documentation drafting.</li>
        <li>Streaming API responses for long-running completions.</li>
        <li>Fine-grained per-team quotas and cost attribution.</li>
        <li>Interactive “review suggestions” mode integrated into the docs editor.</li>
      </ul>
    </section>

    <!-- 8. Security -->
    <section id="security">
      <h2 id="8-security-compliance">8. Security &amp; Compliance</h2>

      <p>
        LexiMind Assistant is deployed in the internal cloud environment and inherits the organisation’s baseline security controls.
        The following considerations apply specifically to this service.
      </p>

      <h3>8.1 Authentication &amp; Authorisation</h3>

      <ul>
        <li>All requests must include a signed JWT or API key issued via the internal IAM system.</li>
        <li>Per-project and per-team roles determine which knowledge sources can be accessed (e.g. restricted Confluence spaces).</li>
        <li>Service-to-service calls are mutually authenticated using mTLS.</li>
      </ul>

      <h3>8.2 Data Handling</h3>

      <ul>
        <li>Requests and responses are encrypted in transit (TLS 1.2+).</li>
        <li>Selected fields are anonymised or redacted in logs (emails, ticket IDs, user names where possible).</li>
        <li>Training and evaluation datasets are curated from opt-in interactions only.</li>
      </ul>

      <h3>8.3 Compliance &amp; Retention</h3>

      <ul>
        <li>Trace and log retention configurable per environment (default 90 days in production).</li>
        <li>Ability to delete user-specific history on request (right to erasure support).</li>
        <li>Regular access reviews for teams with permissions to query production logs.</li>
      </ul>
    </section>

    <!-- 9. Appendix -->
    <section id="appendix">
      <h2 id="9-appendix">9. Appendix</h2>

      <h3>9.1 Related Documents</h3>
      <ul>
        <li><em>LexiMind – Model Evaluation Plan</em> (internal)</li>
        <li><em>LexiMind – Content Policy &amp; Safety Guidelines</em> (internal)</li>
        <li><em>LexiMind – Docs Portal Integration Guide</em> (internal)</li>
      </ul>
    </section>

    <!-- NAV BUTTONS -->
 <p style="margin-top: 2rem;">
  <a href="../../previous-page.html" class="btn">← Previous Page</a>
  <a href="../../next-page.html" class="btn" style="margin-left:0.5rem;">Next Page →</a>
  <a href="../../portfolio.html" class="btn" style="margin-left:0.5rem;">Back to Portfolio</a>
</p>

<footer>
  © 2025 Leslie Amadi — All Rights Reserved.
</footer>

</div>

<!-- Scroll back to top button -->
<a href="#top" class="to-top">↑ Top</a>
</div>



  </main>
</body>
</html>
