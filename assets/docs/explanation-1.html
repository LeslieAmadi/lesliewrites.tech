<!DOCTYPE html>
<html lang="en-GB">
<head>
<meta charset="UTF-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>Explanation | Explainable AI | Leslie Amadi</title>
<link rel="stylesheet" href="../styles.css">
</head>

<body>

<header class="navbar">
  <div class="navbar-inner">
    <div class="logo">Leslie Amadi</div>
    <nav class="nav-menu">
      <a href="../../index.html">Home</a>
      <a href="../../portfolio.html" class="active">Portfolio</a>
      <a href="../../blog.html">Blog</a>
      <a href="../../about.html">About</a>
      <a href="../../contact.html">Contact</a>
    </nav>
  </div>
</header>

<div class="tag-row">
  <span class="tag">Explanation</span>
  <span class="tag">Explainable AI</span>
  <span class="tag">Clinical Systems</span>
  <span class="tag">Trust Architecture</span>
</div>

<header class="doc-header">
  <h1>Explainable AI in Clinical Systems</h1>
  <p class="doc-subtitle">
    How intelligent systems become understandable, trustworthy, and safe to use in human-critical environments.
  </p>
</header>

<div class="breadcrumb-nav">
  <a href="../../portfolio.html" class="btn-secondary btn">← Back to Portfolio</a>
</div>

<main class="content-shell">

<nav class="inline-toc">
  <strong>On this page:</strong>
  <a href="#meaning">Meaning</a> ·
  <a href="#trust">Trust</a> ·
  <a href="#design">Design</a> ·
  <a href="#systems">Systems Thinking</a>
</nav>

<section id="meaning">
  <h2>From Prediction to Meaning</h2>
  <p>
    Explainable AI is not primarily a technical feature.  
    It is a human requirement.
  </p>

  <p>
    In clinical environments, predictions alone are not useful.  
    Numbers alone do not guide decisions.  
    Outputs alone do not create safety.
  </p>

  <p>
    Explainability transforms prediction systems into interpretable decision-support systems —
    systems that do not merely calculate, but communicate.
  </p>
</section>

<section id="trust">
  <h2>How Trust Is Actually Formed</h2>

  <p>
    Trust in clinical systems is not generated by accuracy alone.
  </p>

  <p>
    It emerges from:
  </p>

  <ul>
    <li>Visibility of reasoning</li>
    <li>Consistency of behaviour</li>
    <li>Accountability of outcomes</li>
    <li>Clarity of system limits</li>
  </ul>

  <p>
    Explainable AI makes these qualities visible.
  </p>

  <p>
    It allows clinicians to see not only <em>what</em> the system predicts,
    but <em>why</em> it predicts it.
  </p>
</section>

<section id="design">
  <h2>Explainability as Design Discipline</h2>

  <p>
    In mature systems, explainability is not added after the model is built.
  </p>

  <p>
    It is designed into the system from the beginning:
  </p>

  <ul>
    <li>Data structures are designed for interpretability</li>
    <li>Models are selected for transparency</li>
    <li>Outputs are shaped for human comprehension</li>
    <li>Interfaces are built for trust, not automation</li>
  </ul>

  <p>
    Explainability becomes architecture, not decoration.
  </p>
</section>

<section id="systems">
  <h2>Explainability as System Relationship</h2>

  <p>
    Explainable AI is not a relationship between a user and a model.
  </p>

  <p>
    It is a relationship between:
  </p>

  <ul>
    <li>Humans and systems</li>
    <li>Judgement and automation</li>
    <li>Responsibility and technology</li>
    <li>Decision and consequence</li>
  </ul>

  <p>
    In clinical systems, explainability is not optional —
    it is ethical infrastructure.
  </p>
</section>

</main>

</body>
</html>
