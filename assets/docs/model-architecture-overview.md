# Model Architecture Overview

This model uses a Transformer-based architecture designed to process and generate human-like text.

## Key Components

- **Encoder**: Reads and understands the input text.
- **Decoder**: Generates responses based on context.
- **Attention Mechanism**: Allows the model to focus on the most relevant words in a sentence.
- **Training Data**: The model was trained on a diverse dataset of internet text.

## Why It Matters
This architecture enables the model to understand relationships between words and generate coherent, context-aware text.

# The Transformer Model

Transformers are deep learning models that understand language by using a mechanism called **self-attention**.

Instead of reading words one by one (like RNNs), Transformers process entire sentences at once, which makes them faster and more accurate for large text tasks.

They’re used in modern AI tools like GPT (ChatGPT), BERT, and T5.

## Visual Summary
Input Text → Encoder → Attention → Decoder → Output Text


# Image Recognition Model

This model uses a neuronal network based architecture specialised in analysing images  

## Key Concepts
